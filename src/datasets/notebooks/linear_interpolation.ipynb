{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import griddata, LinearNDInterpolator\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from src.dataset.tools.dataloaders import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 50\n",
    "target_scan = 1\n",
    "workers = 4\n",
    "method = \"lstsq\"  # or \"lstsq\"\n",
    "interpolation_dim = \"2\"  # or \"3\"\n",
    "\n",
    "project_root = Path(\".\").absolute().parents[1]\n",
    "\n",
    "results_path = project_root / \"results\" / \"linear_interpolation\" \n",
    "results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_csv = project_root / \"dataset\" / \"synth_crptn\" / \"150\" / \"train.csv\"\n",
    "train_dataloader = get_dataloader(train_csv, batch_size, workers)\n",
    "\n",
    "tile_csv = project_root / \"dataset\" / \"synth_crptn\" / \"big_tile_no_overlap\" / \"big_tile_dataset.csv\"\n",
    "tile_dataloader = get_dataloader(tile_csv, batch_size, workers, drop_last=False)\n",
    "\n",
    "# setting idx to 0 allows for perfect interpolation -- sanity check\n",
    "def interp2d(data, idx=1):\n",
    "    return np.concatenate([\n",
    "            griddata(\n",
    "                n[idx:, :2],\n",
    "                n[idx:, 3],\n",
    "                n[0, :2]\n",
    "            ) for n in data])\n",
    "\n",
    "def interp3d(data, idx=1):\n",
    "    return np.concatenate([\n",
    "            LinearNDInterpolator(\n",
    "                n[idx:, :3],\n",
    "                n[idx:, 3]\n",
    "           )(n[0, :3]) for n in data])\n",
    "\n",
    "if interpolation_dim == \"2\":\n",
    "    interp_func = interp2d\n",
    "if interpolation_dim == \"3\":\n",
    "    interp_func = interp3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0\n",
    "with torch.no_grad():\n",
    "    dataset = np.empty((0, 5))\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        data, h_target, i_target = batch\n",
    "        \n",
    "        interpolation = interp_func(data, idx=1)\n",
    "        \n",
    "        s_scan = data[:, 1, 8]\n",
    "        t_scan = data[:, 0, 8]\n",
    "        \n",
    "        new_batch = np.stack((\n",
    "            interpolation, \n",
    "            i_target.numpy(), \n",
    "            h_target.numpy(), \n",
    "            s_scan.numpy(), \n",
    "            t_scan.numpy())).T\n",
    "        \n",
    "        nans = np.where(np.isnan(interpolation))\n",
    "        new_batch = np.delete(new_batch, nans, axis=0)\n",
    "        \n",
    "        loss = np.mean(np.abs(new_batch[:, 0] - new_batch[:, 1]))\n",
    "        \n",
    "        running_loss += loss * batch_size\n",
    "        total_loss = running_loss / (((batch_idx+1)* batch_size))\n",
    "        pbar.set_postfix({\n",
    "            'icurr':f\"{loss:.3f}\", \n",
    "            \"itotl\":f\"{total_loss:.3f}\"})\n",
    "        \n",
    "        dataset = np.concatenate((dataset, new_batch))\n",
    "        \n",
    "        if len(dataset) > 50000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.mean(np.abs(dataset[:, 0] - dataset[:, 1]))\n",
    "print(\"Interpolation Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(len(dataset), 5000)\n",
    "ds = dataset[sample]\n",
    "dsx = ds[:, 0]\n",
    "dsy = ds[:, 1]\n",
    "xy = np.vstack([dsy, dsx])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "idx = z.argsort()\n",
    "dsx, dsy, z = dsx[idx], dsy[idx], z[idx]\n",
    "plt.scatter(ds[:, 0], ds[:, 1], s=10, c=z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonization\n",
    "dataset_f = dataset[dataset[:, 4] == 1]  # filter out source-source scans\n",
    "sources = np.unique(dataset_f[:, 3])  # create list of source scans\n",
    "print(sources)\n",
    "source_counts = []\n",
    "for s in sources:\n",
    "    count = len(dataset_f[dataset_f[:, 3]==s])\n",
    "    source_counts.append(count)\n",
    "\n",
    "print(source_counts)\n",
    "    \n",
    "transforms = {}\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "fig, ax = plt.subplots(2, 5)\n",
    "for i, s in enumerate(sources):\n",
    "    if i == 10:\n",
    "        break\n",
    "    dataset_f_s = dataset_f[dataset_f[:, 3] == s]  # filter on source-target pair\n",
    "    X = dataset_f_s[:, 0] # interpolated value at source for given s\n",
    "    y = dataset_f_s[:, 2]                 # all gt harmonization values for S\n",
    "    \n",
    "    if method is \"lstsq\":\n",
    "        xy = np.vstack([y, X])\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        idx = z.argsort()\n",
    "        X, y, z = X[idx], y[idx], z[idx]\n",
    "        ax.flat[i].scatter(X, y, c=z, s=6)\n",
    "        A = np.vstack([X**3, X**2, X, np.ones(len(X))]).T\n",
    "        v1, v2, v3, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "        transforms[(int(s), 1)] = (v1, v2, v3, c)\n",
    "        idx = X.argsort()\n",
    "        X, y = X[idx], y[idx]\n",
    "        ax.flat[i].plot(X, v1*(X**3)+ v2*(X**2) + v3*X + c, 'r')\n",
    "        ax.flat[i].axis('off')\n",
    "        ax.flat[i].set_title(f\"{int(s)}\")\n",
    "        \n",
    "        \n",
    "    elif method is \"MLP\":\n",
    "        transforms[(int(s), 1)] = MLPRegressor(\n",
    "            (100, 100, 100), random_state=1, max_iter=300).fit(X, y)\n",
    "        \n",
    "        xy = np.vstack([y, X])\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        idx = z.argsort()\n",
    "        X, y, z = X[idx], y[idx], z[idx]\n",
    "        ax.flat[i].scatter(X, y, c=z, s=6)\n",
    "        transforms[(int(s), 1)] = MLPRegressor((100, 100, 100)).fit(X, y)\n",
    "        idx = X.argsort()\n",
    "        X, y = X[idx], y[idx]\n",
    "        ax.flat[i].plot(X, transforms[(int(s), 1)].predict(X), 'r')\n",
    "        ax.flat[i].axis('off')\n",
    "        ax.flat[i].set_title(f\"{int(s)}\")\n",
    "    else:\n",
    "        exit(f\"No method: {method}\")\n",
    "        \n",
    "plt.show()\n",
    "print(method)\n",
    "print(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0\n",
    "import code\n",
    "with torch.no_grad():\n",
    "    fixed_tile = np.empty((0, 11), dtype=np.float64)\n",
    "    pbar = tqdm(tile_dataloader, total=len(tile_dataloader))\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        data, h_target, i_target = batch\n",
    "\n",
    "        tile_data = data[:, 0, :].numpy()\n",
    "        \n",
    "        intensity = tile_data[:, 3]\n",
    "        \n",
    "        source_scan = int(data[0, 0, 8])\n",
    "        t = transforms[(source_scan, target_scan)]\n",
    "        \n",
    "        if method is \"lstsq\":\n",
    "            fixed_intensity = (t[0]*(intensity**3)) + (t[1]*(intensity**2)) + (t[2]*intensity) + t[3]\n",
    "        \n",
    "#         if method is \"MLP\":\n",
    "#             new_intensity = t.predict(intensity).reshape(-1, 1)\n",
    "            \n",
    "        tile_data = np.concatenate((\n",
    "            tile_data[:, :3], # include gt harmonization\n",
    "            h_target.numpy().reshape(-1, 1),\n",
    "            tile_data[:, 3].reshape(-1, 1),\n",
    "            fixed_intensity.reshape(-1, 1),\n",
    "            tile_data[:, 4:]), axis=1)\n",
    "        \n",
    "        loss = np.mean(np.abs(tile_data[:, 5] - tile_data[:, 3]))\n",
    "        \n",
    "        running_loss += loss * batch_size\n",
    "        total_loss = running_loss / (((batch_idx+1) * batch_size))\n",
    "        pbar.set_postfix({\n",
    "            \"hcur\": f\"{float(loss):.3f}\",\n",
    "            \"htot\": f\"{float(total_loss):.3f}\"\n",
    "        })\n",
    "        \n",
    "        fixed_tile = np.concatenate((fixed_tile, tile_data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    ax.flat[i].scatter(fixed_tile[:, 0], fixed_tile[:, 1], c=fixed_tile[:, 3+i], s=.5, vmin=0, vmax=1)\n",
    "    ax.flat[i].axis('off')\n",
    "    \n",
    "ax.flat[0].set_title(\"Ground Truth\")\n",
    "ax.flat[1].set_title(\"Corrupted\")\n",
    "ax.flat[2].set_title(\"Fixed\")\n",
    "    \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(tile_csv.parents[0] / f\"fixed_li_{method}.txt.gz\", fixed_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
