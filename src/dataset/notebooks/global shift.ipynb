{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from laspy.file import File\n",
    "from src.dataset.tools.apply_rf import ApplyResponseFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, Array\n",
    "from tqdm import tqdm\n",
    "from pptk import viewer\n",
    "from src.evaluation.histogram_matching import hist_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scans = Path(\"../dublin/npy\")\n",
    "base_train_dataset = Path(\"../synth_crptn/150/neighborhoods\")\n",
    "base_eval_tile = Path(\"../synth_crptn/big_tile_no_overlap/neighborhoods\")\n",
    "gis_train_dataset = Path(\"../synth_crptn+shift/150/neighborhoods\")\n",
    "gis_eval_tile = Path(\"../synth_crptn+shift/big_tile_no_overlap/neighborhoods\")\n",
    "\n",
    "gis_train_dataset.mkdir(parents=True, exist_ok=True)\n",
    "gis_eval_tile.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sigmoid(x, h=0, v=0, s=1, l=1):\n",
    "    return (s/(1 + np.exp(-l*(x-h)))) + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 314307.449 317707.233\n",
      "Y: 233031.721 235884.832\n",
      "Z: -144.24200000000002 385.369\n"
     ]
    }
   ],
   "source": [
    "min_x = min_y = min_z = 99999999\n",
    "max_x = max_y = max_z = 1\n",
    "\n",
    "min_intensity = 0\n",
    "max_intensity = 512\n",
    "\n",
    "for pc in base_scans.glob(\"*.npy\"):\n",
    "    f = np.load(pc)\n",
    "    if f[:, 0].min() < min_x:\n",
    "        min_x = f[:, 0].min()\n",
    "    if f[:, 0].max() > max_x:\n",
    "        max_x = f[:, 0].max()\n",
    "    if f[:, 1].max() < min_y:\n",
    "        min_y = f[:, 1].min()\n",
    "    if f[:, 1].max() > max_y:\n",
    "        max_y = f[:, 1].max()\n",
    "    if f[:, 2].min() < min_z:\n",
    "        min_z = f[:, 2].min()\n",
    "    if f[:, 2].max() > max_z:\n",
    "        max_z = f[:, 2].max()\n",
    "\n",
    "print(\"X:\", min_x, max_x)\n",
    "print(\"Y:\", min_y, max_y)\n",
    "print(\"Z:\", min_z, max_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1993653/1993653 [27:10<00:00, 1222.85it/s]\n",
      "100%|██████████| 1000000/1000000 [14:12<00:00, 1173.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../synth_crptn/big_tile_no_overlap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/david/.conda/envs/lidar/lib/python3.7/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def proc(patch_path):\n",
    "    f = np.loadtxt(patch_path)\n",
    "    x = f[:, 0]\n",
    "    x = (x - min_x)/(max_x - min_x)\n",
    "\n",
    "    floor = .3  # lower bound of sigmoid\n",
    "    center = .5 # where the middle is\n",
    "    #  the first point is the source gt center, \n",
    "    # the second point is the source alt center\n",
    "    # the 150 neighborhood comes from the target\n",
    "    f[:, 3] = f[:, 3] * sigmoid(x, h=center, v=floor, l=100, s=1-floor)\n",
    "    np.savetxt(gis_train_dataset/f\"{patch_path.stem + patch_path.suffix}\", f)\n",
    "\n",
    "def proc2(patch_path):\n",
    "    f = np.loadtxt(patch_path)\n",
    "    x = f[:, 0]\n",
    "    x = (x - min_x)/(max_x - min_x)\n",
    "\n",
    "    floor = .3  # lower bound of sigmoid\n",
    "    center = .5 # where the middle is\n",
    "    f[:, 3] = f[:, 3] * sigmoid(x, h=center, v=floor, l=100, s=1-floor)\n",
    "    np.savetxt(gis_eval_tile/f\"{patch_path.stem + patch_path.suffix}\", f)\n",
    "    \n",
    "def proc3(tile_path):\n",
    "    f = np.load(tile_path)\n",
    "    x = f[:, 0]\n",
    "    x = (x - min_x)/(max_x - min_x)\n",
    "\n",
    "    floor = .3  # lower bound of sigmoid\n",
    "    center = .5 # where the middle is\n",
    "    f[:, 3] = f[:, 3] * sigmoid(x, h=center, v=floor, l=100, s=1-floor)\n",
    "    np.savetxt(gis_eval_tile.parents[0]/f\"{tile_path.stem + tile_path.suffix}\", f)\n",
    "    \n",
    "pool = Pool(processes=8)\n",
    "# convert training patches\n",
    "patches = [f for f in base_train_dataset.glob(\"*.txt.gz\")]\n",
    "for _ in tqdm(pool.imap_unordered(proc, patches), total=len(patches)):\n",
    "    pass \n",
    "\n",
    "# convert eval patches\n",
    "patches = [f for f in base_eval_tile.glob(\"*.txt.gz\")]\n",
    "for _ in tqdm(pool.imap_unordered(proc2, patches), total=len(patches)):\n",
    "    pass \n",
    "\n",
    "# convert eval gt and alt tiles\n",
    "tile_path = base_eval_tile.parents[0]\n",
    "print(tile_path)\n",
    "for tile in [\"gt.npy\", \"alt.npy\"]:\n",
    "    proc3(tile_path / tile)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1993653/1993653 [01:49<00:00, 18254.04it/s]\n",
      "100%|██████████| 81782/81782 [00:03<00:00, 23407.73it/s]\n",
      "100%|██████████| 20299/20299 [00:00<00:00, 23442.63it/s]\n",
      "100%|██████████| 25374/25374 [00:01<00:00, 23515.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# copy over csv files, update paths -- target intensities will be wrong :P\n",
    "import pandas as pd\n",
    "\n",
    "# training\n",
    "for csv in [\"master.csv\", \"train.csv\", \"val.csv\", \"test.csv\"]:\n",
    "    base_csv = pd.read_csv(str(base_train_dataset.parents[0] / csv))\n",
    "    new_csv_path = gis_train_dataset.parents[0] / csv\n",
    "    \n",
    "    gis_examples = [None] * len(base_csv)\n",
    "    for i in tqdm(range(len(gis_examples))):\n",
    "        file = Path(base_csv.examples[i])\n",
    "        name = file.stem + \".gz\"\n",
    "        new_file = file.parents[3] / \"synth_crptn+shift/150/neighborhoods\" / name \n",
    "        if not new_file.exists():\n",
    "            exit(f\"file not found: {new_file}\")\n",
    "        gis_examples[i] = str(new_file)\n",
    "\n",
    "    base_csv.examples = gis_examples\n",
    "    base_csv.to_csv(f\"../synth_crptn+shift/150/{csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1699/1000000 [00:00<00:58, 16985.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'examples', 'source_scan', 'target_intensity'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:56<00:00, 17748.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "base_csv = pd.read_csv(str(base_eval_tile.parents[0] / \"big_tile_dataset.csv\"))\n",
    "new_csv_path = gis_eval_tile.parents[0] / \"big_tile_dataset.csv\"\n",
    "\n",
    "gis_examples = [None] * len(base_csv)\n",
    "for i in tqdm(range(len(gis_examples))):\n",
    "    file = Path(base_csv.examples[i])\n",
    "    name = file.stem + \".gz\"\n",
    "    new_file = file.parents[3] / \"synth_crptn+shift/big_tile_no_overlap/neighborhoods\" / name \n",
    "    if not new_file.exists():\n",
    "        exit(f\"file not found: {new_file}\")\n",
    "    gis_examples[i] = str(new_file)\n",
    "\n",
    "base_csv.examples = gis_examples\n",
    "base_csv.to_csv(f\"../synth_crptn+shift/big_tile_no_overlap/big_tile_dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the ref scan\n",
    "# from pptk import viewer\n",
    "# ref = 1\n",
    "# gt_pc = np.load(flight_paths / f\"{ref}.npy\")[:, :4]\n",
    "# shift_pc = np.load(gis_flight_paths / f\"{ref}.npy\")[:, 3]\n",
    "\n",
    "\n",
    "# v = viewer(gt_pc[:, :3])\n",
    "# attr1 = gt_pc[:, 3]\n",
    "# attr2 = shift_pc\n",
    "# v.attributes(attr1, attr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
